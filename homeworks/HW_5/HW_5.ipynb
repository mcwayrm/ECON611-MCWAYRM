{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"https://images.efollett.com/htmlroot/images/templates/storeLogos/CA/864.gif\" style=\"float: right;\"> \n",
    "\n",
    "# Econ 611\n",
    "## HW 5\n",
    "In this assignment, you will not be using jupyter notebooks. We have put starter code to answer the hw questions. <b>For every function you write, make sure to comment what it does in the proper format. In addition, unless told otherwise, you may not use modules to answer questions on your homeworks</b>.\n",
    "\n",
    "### Warm-up: \n",
    "This should be done in a file called ```warm_up.py```  \n",
    "#### Write a function ```factorial(x)``` that finds the factorial of an integer x\n",
    "For example, ```factorial(4)``` should output 24.\n",
    "\n",
    "### Main Questions\n",
    "#### Write a function gradient_optimizer that uses a gradient to find a max/min of a function\n",
    "This should be done in the file ```gradient_optimizer```\n",
    "A gradient $\\nabla f$ is the multivariate derivative of f. It has the unique propert that for any set of points $(x_1,...,x_n)$, $\\nabla f(x_1,...,x_n)$ points towards an extremum. Therefore, you can find a max/min of f by starting with an initial guess of $(x_1,...,x_n)$ and iteratively updating it with the gradient. Therefore, the  equation to optimize using gradient is\n",
    "\\begin{equation}\n",
    "(x_1,...,x_n)_{new} = (x_1,...,x_n)_{old} + t * \\nabla f(x_1,...,x_n)_{old}\n",
    "\\end{equation}\n",
    "\n",
    "where $t$ is a small scalar to control the size of the gradient when we take a step. At an extremum, $\\nabla f = 0$. Since the sequence of $x$s (call it $x_n$) converges to an extremum, $\\nabla x_n$ definitely converges to $0$ as long as the gradient is continuous. The definition of convergence to 0 is that at some point N, for any $n \\geq N$, $|x_n-0| < \\epsilon$. <b> We can assume $f$ is close enough to an extremum if $\\nabla x_{new} < \\epsilon$ </b>. Therefore, the algorithm to find the extremum is\n",
    "```\n",
    "xn = [0,...,0] # The length of list is the number of variables there are in the function\n",
    "for i in range(max_iter):\n",
    "    grad = gradient(f, xn)\n",
    "    if all values in grad < eps:\n",
    "        return xn\n",
    "    xn = xold + gradient(f, xn)\n",
    "    \n",
    "grad = gradient(f, xn)\n",
    "if all values in grad < eps:\n",
    "    return xn\n",
    "else:\n",
    "    return None\n",
    "```\n",
    "  \n",
    "\n",
    "```gradient_optimizer``` takes in 5 arguments. \n",
    "1. ```f``` is the function that we are optimizing. A function can be put in as a parameter by creating a function that is in the equation. For example, 2 possible ways to create a function for $f(x_1, x_2) = x_1^2x_2^2$ are:\n",
    "```\n",
    "lambda x: x ** 2\n",
    "```  \n",
    "where x is a numpy array of dimensions (1,2), and  \n",
    "```\n",
    "def f(x):\n",
    "    return x ** 2\n",
    "```\n",
    "For simplicity, assume $f: \\mathbb{R} \\mapsto \\mathbb{R}$ is differentiable. \n",
    "2. ```x0``` is the starting x value. This should be a numpy array.\n",
    "3. ```eps``` is a <i>small</i> number that you use to test that $x_n$ is close to 0\n",
    "4. ```t``` is the scalar described above\n",
    "5. ```num_iter``` is the maximum number of iterations the function is allowed to make before it decides there is no extremum\n",
    "Make sure to account for possible errors in the input that would cause your program to crash\n",
    "\n",
    "You can use the function https://docs.scipy.org/doc/scipy-0.19.1/reference/generated/scipy.optimize.approx_fprime.html to approximate the gradient or use the algorithm described in the first bonus opportunity  \n",
    "  \n",
    "\n",
    "<b>BONUS</b> Approximate the gradient.  \n",
    "At a point $(c_1,...,c_n)$,\n",
    "\\begin{equation}\n",
    "\\frac{\\partial f}{\\partial x_i} = lim_{x_i \\to c_i} \\frac{f(c_1,...,x_i,...,c_n) - f(c_1,...,c_i,...,c_n)}{(c_1,...,x_i,...,c_n) - (c_1,...,c_i,...,c_n)}\n",
    "\\end{equation}\n",
    "This means that \n",
    "\\begin{equation}\n",
    "\\frac{\\partial f}{\\partial x_i} \\approx \\frac{f(x_1,...,x_i+\\epsilon,...x_n) - f(x_1,...,x_i - \\epsilon,...,x_n)}{2\\epsilon}\n",
    "\\end{equation}\n",
    "Therefore, the gradient of f at c is approximately\n",
    "\\begin{equation}\n",
    "\\nabla f(c_1,...,c_n) \\approx [\\frac{f(c_1+\\epsilon,...,c_n) - f(c_1-\\epsilon,...,c_n)}{2\\epsilon}, ..., \\frac{f(c_1,...,c_n+\\epsilon) - f(c_1,...,c_n-\\epsilon)}{2\\epsilon}]\n",
    "\\end{equation}\n",
    "We can use this fact to estimate the gradient with the following algorithm\n",
    "```\n",
    "grad = []\n",
    "for i in len(x):\n",
    "    der = (f(xpluseps) - f(xminuseps))/(2 * eps)\n",
    "    grad.append(der)\n",
    "```\n",
    "  \n",
    "\n",
    "<b> BONUS</b>: PCA is a data dimensionality reduction technique. For an $nxm$ dimension matrix, there are $min(n-1, m)$ principal components, and they are ordered so that the higher principal component explains more of the variance in the data (the first principal component explains more variance than the second...). One reason PCA is used is to reduce the dimension of the data to 1 or 2 dimensions so it can be plotted.  \n",
    "  \n",
    "Use PCA (https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) to grab the first principal component for each iteration of x and make a scatter plot of iteration vs 1st principal component of x. Make your plot using the module ```matplotlib```\n",
    "\n",
    "<b>Examples:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "zero for f(x) = x1^2x2^2x3^2: [3.2831997634823315e-05, 4.9247996452291656e-05, 0.0]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a1f4e7cd0>]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from hw4_key import gradient_optimizer_key, plot\n",
    "solution, xs = gradient_optimizer_key(lambda x: [xi**2 for xi in x], [10, 15, 0])\n",
    "print(f'zero for f(x) = x1^2x2^2x3^2: {solution}')\n",
    "plot(xs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now create a class ```cross-entropy_optimizer```\n",
    "This should be done in a file ```cross-entropy.py```.  \n",
    "##### Background\n",
    "Suppose you have two sets of data: an nx1 matrix $Y$ that has binary values (all values in $Y$ are 1 or 0) and an nxk matrix $X$. You want to fit a kx1 matrix $\\beta$ so that for each row $i \\in n$, $\\frac{1}{1+e^{-x_i\\beta}}$ is as close to $Y_i$ as possible. Below is the graph of $\\frac{1}{1+e^{c}}$.\n",
    "<img src = \"sigmoid.png\">\n",
    "As can be seen, when $x_i\\beta$ is very positive, $\\frac{1}{1+e^{-x_i\\beta}}$ is close to 1 and when $x_i\\beta$ is very negative, $\\frac{1}{1+e^{-x_i\\beta}}$ is close to 0.  \n",
    "  \n",
    "The function you will are trying to optimize is\n",
    "\\begin{equation}\n",
    "L(\\beta) = \\sum_{i=1}^{n} y_ilog(\\frac{1}{1+e^{-x_i\\beta}}) + (1 - y_i)log(1 -  \\frac{1}{1+e^{-x_i\\beta}})\n",
    "\\end{equation}\n",
    "\n",
    "##### Initialization\n",
    "The ```__init__``` input parameters should be 2 numpy arrays ```X``` and ```Y```. You do not need to worry about the ```False``` case. Inside the ```__init__```, you should create a class variable named ```beta``` that is a numpy array of zeros with shape (num X columns, 1)\n",
    "\n",
    "##### Create a function ```sigmoid```\n",
    "This function will take in two variables ```X``` and ```beta``` and return $\\frac{1}{1+e^{-X\\beta}}$ (Note: the output to this is the same as if you ran $\\frac{1}{1+e^{-x_i\\beta}}$ on each $i$\n",
    "\n",
    "##### Create a function```:```\n",
    "This function will take in three variables: ```X```, ```Y```, and ```beta```, and it will return $-\\sum_{i=1}^{n} y_ilog(\\frac{1}{1+e^{-x_i\\beta}}) + (1 - y_i)log(1 -  \\frac{1}{1+e^{-x_i\\beta}})$. Bonus: Do this only using numpy functions (no for loops).\n",
    "\n",
    "##### Create a function ```fit_approx```\n",
    "Import your gradient optimizer function that you created in the other file. Run it using ```cross_entropy``` as ```f``` and class variable ```beta``` as ```x0```. Reset ```beta``` as the output.\n",
    "\n",
    "##### Create a function ```fit```\n",
    "The actual gradient of this cross-entropy function is $X^T(Y-\\frac{1}{1+e^{-X\\beta}})$. Using the gradient optimizing algorithm from above, solve to find the optimal $\\beta$ and set it as a class variable ```beta_2```. For your convenience, here is the algorithm:\n",
    "\n",
    "```\n",
    "beta_2 = zeros\n",
    "for i in num_epochs:\n",
    "    p = sigmoid(X, beta_2)\n",
    "    grad = X.T(Y-p)\n",
    "    beta_2 = beta_2 - t * grad\n",
    "```\n",
    "  \n",
    "  \n",
    "I have provided you with sample X and Y as well as a function to plot the data and the classification line from the two betas. Make sure that ```fit``` and ```fit_approx``` have close to the same values for $\\beta$, and the classification line is between the two classes. Below is a sample output for the graph.\n",
    "<img src = \"region.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "gradient() missing 2 required positional arguments: 'f' and 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-70-f95ff6af9479>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[0mnew_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m \u001b[0mnew_func\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgradient_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-70-f95ff6af9479>\u001b[0m in \u001b[0;36mgradient_optimizer\u001b[1;34m(self, eps, t, num_iter)\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m             \u001b[1;31m# Gradient with updated X\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m             \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[1;31m# Check if within acceptable range\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0meps\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mvalue\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: gradient() missing 2 required positional arguments: 'f' and 'x'"
     ]
    }
   ],
   "source": [
    "Xs = [1,2,3,4]\n",
    "f = lambda y: y ** 2\n",
    "\n",
    "class gradient_optimizer:\n",
    "    def __init__(self, f, x):\n",
    "        self.f = f\n",
    "        self.x = x\n",
    "    \n",
    "    def gradient(self):\n",
    "        '''Calculate derivations of each element in x, store gradient'''\n",
    "        grad = []\n",
    "        for current_x in self.x:\n",
    "            xplus_eps = current_x + eps\n",
    "            xminus_eps = current_x - eps\n",
    "            derv = (self.f(xplus_eps) - self.f(xminus_eps))/(2 * eps)\n",
    "            grad.append(derv)\n",
    "        return grad\n",
    "    def gradient_optimizer(self, eps = 0.0001, t = 0.01, num_iter = 1000):\n",
    "        '''Gradient Optimizer over numerous iterations'''\n",
    "        for i in range(num_iter):\n",
    "            # Gradient with updated X\n",
    "            grad = gradient()\n",
    "            # Check if within acceptable range\n",
    "            if all(value < eps for value in grad):\n",
    "                return self.x\n",
    "            # Scalar Step\n",
    "            t_into_grad = [t * grads for grads in gradient()]\n",
    "            # Subtract each gradient from Xs, update X vector\n",
    "            self.x = [x_old - g for x_old, g in zip(self.x, t_into_grad)]\n",
    "\n",
    "new_func = gradient_optimizer(f, Xs)\n",
    "new_func.gradient_optimizer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "marshmellow is such a good boy!\n",
      "Annie, you should probably walk your dog, Tigger, since he is such a good boy. \n",
      "You should go Now!\n",
      "Ryan, you should probably walk your dog, Marshmellow, since he is such a good boy. \n",
      "You should go later...\n",
      "\n",
      "Test\n",
      "Annie, you should probably walk your dog, Spot, since he is such a good boy. \n",
      "You should go Now!\n"
     ]
    }
   ],
   "source": [
    "## Practice for Classes\n",
    "class Dog:\n",
    "    def __init__(self, name = 'Spot', age = 2):\n",
    "        # This is what every dog will have. What makes up a dog\n",
    "        self.name = name\n",
    "        self.age = age # these assign values to characterize the dog\n",
    "        # I would like to have this return statement but can't have a return statment in the __init__\n",
    "        #return \"What a good boy!\"\n",
    "    def __str__(self):\n",
    "        # But I can get that return statement from before here though\n",
    "        return self.name+ ' is such a good boy!'\n",
    "    def walk(self, owner = 'Annie', now = 'Now!'):\n",
    "        # This is an optional function that you could pass, unlike _init_ which will be passed for every dog defined by class Dog()\n",
    "        return print(owner.title() + \", you should probably walk your dog, \" + self.name.title() + \", since he is such a good boy. \\nYou should go \" + now)\n",
    "\n",
    "# Assigning characteristics to a dog as they are traits from their _init_\n",
    "marshmellow = Dog('marshmellow', .5)\n",
    "print(marshmellow.__str__())\n",
    "# New dog, who is about to go on a walk\n",
    "tigger = Dog('tigger', 6)\n",
    "tigger.walk('Annie')\n",
    "# Or maybe you want to go on a walk later\n",
    "marshmellow.walk('Ryan', 'later...')\n",
    "print('\\nTest')\n",
    "# No arguments, and calling a function within a class in one line. This is optimally what we would want\n",
    "spot_walk = Dog().walk()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import scipy\n",
    "from scipy import optimize\n",
    "\n",
    "#gradient optimizer\n",
    "def gradient_optimizer(f, x0, eps=.0001, t = .01, num_iter=1000000):\n",
    "    for i in range(num_iter):\n",
    "        grad = optimize.approx_fprime(x0, f, eps)\n",
    "        if all(z < eps for z in grad):\n",
    "            x0 = x0 - t * grad\n",
    "            return x0 \n",
    "        else:\n",
    "            x0 = x0 - t * grad\n",
    "            grad = optimize.approx_fprime(x0, f, eps)\n",
    "# Class\n",
    "class Cross_Entropy:\n",
    "    '''The Cross Entropy Class'''\n",
    "    def __init__(self, x, y):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.beta = np.zeros((x.shape[1], 1))\n",
    "    \n",
    "    def sigmoid(self):\n",
    "        '''The Sigmoid form for the log likelihood function''' \n",
    "        return 1 / (1 + np.exp(np.dot(-self.x, self.beta)))\n",
    "    \n",
    "    def reg(self):\n",
    "        '''The Log Likelihood Function'''\n",
    "        return -sum(self.y * np.log(self.sigmoid()) + (1 - self.y) * np.log(1 - self.sigmoid()))\n",
    "    \n",
    "    def fit_approx(self):\n",
    "        fitted_approx = gradient_optimizer(self.reg(), self.beta)   \n",
    "        return beta_new\n",
    "    \n",
    "    def fit(self, t = 0.01, num_epochs = 1000):\n",
    "        '''The actual cross gradient provided by Javi'''\n",
    "        beta_2 = np.zeros((self.x.shape[1], 1))\n",
    "        for i in num_epochs:\n",
    "            p = self.sigmoid(self.x, beta_2)\n",
    "            grad = self.x.T(self.y - p)\n",
    "            beta_2 = beta_2 - t * grad\n",
    "        return beta_2\n",
    "\n",
    "def sample_data():\n",
    "    numpos = 100\n",
    "    numneg = 100\n",
    "    \n",
    "    mupos = [1.0, 1.0]\n",
    "    covpos = np.array([[1.0, 0.0], [0.0, 1.0]])\n",
    "    muneg = [-1.0, -1.0]\n",
    "    covneg = np.array([[1.0, 0.0], [0.0, 1.0]])\n",
    "    \n",
    "    Xpos = np.ones((numpos, 3))\n",
    "    for i in range(numpos):\n",
    "        Xpos[i, 0:2] = np.random.multivariate_normal(mupos, covpos)\n",
    "    Ypos = np.ones((numpos, 1))\n",
    "        \n",
    "    Xneg = np.ones((numneg, 3))\n",
    "    for i in range(numneg):\n",
    "        Xneg[i, 0:2] = np.random.multivariate_normal(muneg, covneg)\n",
    "    Yneg = np.zeros((numneg, 1))\n",
    "    \n",
    "    X = np.concatenate((Xpos, Xneg))\n",
    "    Y = np.concatenate((Ypos, Yneg))\n",
    "    \n",
    "    return X, Y, Xpos, Xneg\n",
    "\n",
    "def plotline(beta, Xpos, Xneg):\n",
    "    xVals = np.linspace(-3, 3, 100) # list of 100 evenly spaced numbers between xMin and xMax\n",
    "    yVals = (-beta[0] * xVals - beta[2])/beta[1] # a1x + a2y+a3 = 0, so y = (-a1x-a3)/a2\n",
    "    idxs = np.where((yVals >= -3) & (yVals <= 3))\n",
    "    plt.scatter(Xpos[:,0],Xpos[:,1])\n",
    "    plt.scatter(Xneg[:,0],Xneg[:,1])\n",
    "    plt.plot(xVals[idxs], yVals[idxs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'numpy.ndarray' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-50-9a4f8505e8c7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mtest_run\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCross_Entropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_X\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_Y\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m \u001b[0mtest_fit_approx_beta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_approx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m \u001b[0mtest_fit_beta\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mtest_plot_approx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtest_run\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplotline\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_fit_approx_beta\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_Xpos\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest_Xneg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-5e65073def35>\u001b[0m in \u001b[0;36mfit_approx\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mfit_approx\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m         \u001b[0mfitted_approx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgradient_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     34\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mbeta_new\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-5e65073def35>\u001b[0m in \u001b[0;36mgradient_optimizer\u001b[1;34m(f, x0, eps, t, num_iter)\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mgradient_optimizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m.0001\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iter\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_iter\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m         \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moptimize\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapprox_fprime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mz\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0meps\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mz\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m             \u001b[0mx0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx0\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mt\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mgrad\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36mapprox_fprime\u001b[1;34m(xk, f, epsilon, *args)\u001b[0m\n\u001b[0;32m    754\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    755\u001b[0m     \"\"\"\n\u001b[1;32m--> 756\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0m_approx_fprime_helper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    757\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Roaming\\Python\\Python37\\site-packages\\scipy\\optimize\\optimize.py\u001b[0m in \u001b[0;36m_approx_fprime_helper\u001b[1;34m(xk, f, epsilon, args, f0)\u001b[0m\n\u001b[0;32m    688\u001b[0m     \"\"\"\n\u001b[0;32m    689\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mf0\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 690\u001b[1;33m         \u001b[0mf0\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxk\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    691\u001b[0m     \u001b[0mgrad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    692\u001b[0m     \u001b[0mei\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mxk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: 'numpy.ndarray' object is not callable"
     ]
    }
   ],
   "source": [
    "f = lambda y: y ** 2\n",
    "\n",
    "test_data = sample_data()\n",
    "test_X = test_data[0]\n",
    "test_Y = test_data[1]\n",
    "test_Xpos = test_data[2]\n",
    "test_Xneg = test_data[3]\n",
    "\n",
    "test_run = Cross_Entropy(test_X, test_Y)\n",
    "test_fit_approx_beta = test_run.fit_approx()\n",
    "test_fit_beta = test_run.fit()\n",
    "test_plot_approx = test_run.plotline(test_fit_approx_beta, test_Xpos, test_Xneg)\n",
    "test_plot_fit = test_run.plotline(test_fit_beta, test_Xpos, test_Xneg)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
